{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7869f773-e96f-46ab-a422-1f48d5245ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1: What is the mathematical formula for a linear SVM?\n",
    "\n",
    "# Answer:\n",
    "# The mathematical formula for a linear Support Vector Machine (SVM) is:\n",
    "# \n",
    "# f(x) = w^T x + b\n",
    "# \n",
    "# where:\n",
    "# - w is the weight vector,\n",
    "# - x is the feature vector,\n",
    "# - b is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72054c3d-9ee5-43b4-bb27-2611cbdb4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2: What is the objective function of a linear SVM?\n",
    "\n",
    "# Answer:\n",
    "# The objective function of a linear SVM is to minimize:\n",
    "# \n",
    "# (1/2) * ||w||^2 + C * ∑(xi)\n",
    "# \n",
    "# where:\n",
    "# - ||w||^2 is the regularization term (to minimize the weight vector norm),\n",
    "# - C is the regularization parameter,\n",
    "# - xi are the slack variables that allow some misclassification.\n",
    "# \n",
    "# The regularization parameter C balances the trade-off between maximizing the margin and minimizing the classification error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f496e75-2538-4278-88a1-f7a3a650234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3: What is the kernel trick in SVM?\n",
    "\n",
    "# Answer:\n",
    "# The kernel trick is a method used in Support Vector Machines (SVMs) to handle non-linear relationships between features. It involves transforming the input features into a higher-dimensional space using a kernel function, where a linear separator can be applied.\n",
    "# \n",
    "# Common kernel functions include:\n",
    "# - Polynomial Kernel: K(xi, xj) = (xi^T xj + c)^d\n",
    "# - Radial Basis Function (RBF) Kernel: K(xi, xj) = exp(-γ ||xi - xj||^2)\n",
    "# \n",
    "# The kernel trick allows SVMs to find non-linear decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4163544-4497-4772-884f-f592517fd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q4: What is the role of support vectors in SVM? Explain with an example.\n",
    "\n",
    "# Answer:\n",
    "# Support vectors are the data points that lie closest to the decision boundary (hyperplane) and are crucial in defining the margin. These points directly influence the position and orientation of the hyperplane.\n",
    "# \n",
    "# Example: In a binary classification problem, if we have two classes with some overlapping points, the support vectors are the points that lie on the edge of the overlap region. These points are used to determine the optimal hyperplane.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44dacb-8e11-4b24-a495-7ec56e30f8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q5: Illustrate with examples and graphs of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM.\n",
    "\n",
    "# Answer:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for simplicity\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with hard margin\n",
    "clf_hard = SVC(kernel='linear', C=np.inf)\n",
    "clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Train SVM with soft margin\n",
    "clf_soft = SVC(kernel='linear', C=1.0)\n",
    "clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundaries(X, y, model, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries for Hard Margin SVM\n",
    "plot_decision_boundaries(X_test, y_test, clf_hard, 'Hard Margin SVM')\n",
    "\n",
    "# Plot decision boundaries for Soft Margin SVM\n",
    "plot_decision_boundaries(X_test, y_test, clf_soft, 'Soft Margin SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
