{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f508fcb-c32c-4cfd-bf0f-73bb207ff88b",
   "metadata": {},
   "source": [
    "## Q1. What is an activation function in the context of artificial neural networks?\n",
    "## Q2. What are some common types of activation functions used in neural networks?\n",
    "## Q3. How do activation functions affect the training process and performance of a neural network?\n",
    "## Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
    "## Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
    "## Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
    "## Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
    "## Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
    "## Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf476a-6d30-43d0-869b-ddeb70beb1b9",
   "metadata": {},
   "source": [
    "## ans 1\n",
    "Activation function, which is also known as Tranfer function, decides whether or not to activte the neurons. it is responsible to give output using a set\n",
    "of inputs to the hidden layer or as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da59b4d-ef52-4aa9-bb58-088e4b67388c",
   "metadata": {},
   "source": [
    "## ans 2\n",
    "Some common types of activation functions are-- sigmoid activation function, binary step function, ReLU activation function, leaky ReLU activation function, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6204b3cb-d220-403c-9916-7344442afe5d",
   "metadata": {},
   "source": [
    "## ans 3\n",
    "Activation function affec the training process because they are deciding factors of whether or not to activate th neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48bcc8-a721-4379-b004-25e370de833f",
   "metadata": {},
   "source": [
    "## ans 4\n",
    "Sigmoid activation function takes input as any real values from - infinity to + infinity and the output is binary [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0564852-aff9-433a-afef-106451b80db3",
   "metadata": {},
   "source": [
    "## ans 5\n",
    "ReLU activation function stands for Rectified Linear Unit. The ReLU function does not activate all the neurons at the same time. The neurons will only be deactivated if the output of the linear transformation is less than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc491e6-011f-44a8-b3ab-c724a3a16ca4",
   "metadata": {},
   "source": [
    "## ans 6\n",
    "The advantage of using a ReLU function over a sigmoid function is since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850fe88-d7bf-4912-86a6-bf542cf0dabe",
   "metadata": {},
   "source": [
    "## ans 7\n",
    "Same as that of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. By making this minor modification for negative input values, the gradient of the left side of the graph comes out to be a non-zero value. Therefore, we would no longer encounter dead neurons in that region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102578b-2e85-43f6-abb3-45c0e49ec76f",
   "metadata": {},
   "source": [
    "## ans 8\n",
    "The Softmax function is described as a combination of multiple sigmoids. \n",
    "It calculates the relative probabilities. Similar to the sigmoid/logistic activation function, the SoftMax function returns the probability of each class. \n",
    "It is most commonly used as an activation function for the last layer of the neural network in the case of multi-class classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c639fa1-fded-4891-b43e-60042f5ad36b",
   "metadata": {},
   "source": [
    "## ans 9\n",
    "Very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. The output of the tanh activation function is Zero centred; hence we can easily map the output values as strongly negative, neutral, or strongly positive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
