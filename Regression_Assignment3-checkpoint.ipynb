{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c05eebd-1ac9-42e6-8728-6aa2ccea7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "# \n",
    "# Answer:\n",
    "# Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes\n",
    "# a penalty term based on the absolute values of the coefficients. It minimizes the sum of squared errors while \n",
    "# also shrinking some coefficients to zero, effectively performing feature selection.\n",
    "# \n",
    "# Difference from Other Techniques:\n",
    "# - Ordinary Least Squares (OLS) regression only minimizes the sum of squared residuals without any penalty term.\n",
    "# - Ridge Regression includes a penalty based on the squared values of the coefficients, reducing multicollinearity \n",
    "# but not performing feature selection.\n",
    "# - Lasso Regression can shrink some coefficients to zero, performing both regularization and feature selection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "# \n",
    "# Answer:\n",
    "# The main advantage of Lasso Regression in feature selection is its ability to automatically perform feature selection\n",
    "# by shrinking some coefficients exactly to zero. This results in a simpler model with only the most significant features\n",
    "# being selected, leading to better interpretability and potentially improved performance by excluding less important features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "# \n",
    "# Answer:\n",
    "# In Lasso Regression, non-zero coefficients represent the features that are selected and have a meaningful impact \n",
    "# on the response variable. Coefficients that are shrunk to zero indicate features that are not selected by the model. \n",
    "# The magnitude of non-zero coefficients shows the strength of the relationship between the feature and the target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "# \n",
    "# Answer:\n",
    "# The key tuning parameter in Lasso Regression is the regularization parameter (λ). This parameter controls the strength\n",
    "# of the penalty applied to the coefficients.\n",
    "# - A larger λ increases the penalty, leading to more coefficients being shrunk to zero and resulting in a sparser model.\n",
    "# - A smaller λ results in less regularization, making the model closer to ordinary least squares regression.\n",
    "# \n",
    "# The choice of λ affects the model's complexity and performance, with high λ values potentially causing underfitting and \n",
    "# low λ values potentially causing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "# \n",
    "# Answer:\n",
    "# Lasso Regression itself is designed for linear relationships. However, it can be used for non-linear problems by applying\n",
    "# it to non-linear transformations of the features, such as polynomial or interaction terms. For more complex non-linear\n",
    "# relationships, other techniques such as polynomial regression or non-linear models may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "# \n",
    "# Answer:\n",
    "# Ridge Regression penalizes the sum of the squares of the coefficients (L2 norm), which helps in reducing multicollinearity \n",
    "# but does not perform feature selection. Lasso Regression penalizes the absolute values of the coefficients (L1 norm), \n",
    "# which can shrink some coefficients to zero, performing feature selection along with regularization.\n",
    "# \n",
    "# Ridge is more suitable when we want to prevent multicollinearity without necessarily excluding features, while Lasso \n",
    "# is better when feature selection is also desired.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "# \n",
    "# Answer:\n",
    "# Yes, Lasso Regression can handle multicollinearity to some extent. The L1 penalty applied by Lasso can shrink the \n",
    "# coefficients of correlated features, effectively selecting one feature from a group of correlated ones and setting the\n",
    "# others to zero. This can reduce the impact of multicollinearity by si\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056a0e4-d86c-4291-b748-fb1677d9f3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
