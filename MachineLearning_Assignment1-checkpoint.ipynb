{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd092be-2dc6-4138-9060-de3ae63d191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "# Answer:\n",
    "# Overfitting: Overfitting occurs when a model learns the training data too well, including its noise and outliers, \n",
    "# which leads to poor generalization to new, unseen data. The model performs well on the training data but poorly on the test data.\n",
    "# Consequences: Poor performance on new data, increased complexity, and a model that captures noise rather than underlying patterns.\n",
    "# Mitigation: Use techniques like cross-validation, regularization (L1, L2), pruning (in decision trees), early stopping, and obtaining more data.\n",
    "\n",
    "# Underfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the data. \n",
    "# The model performs poorly on both training and test data because it fails to learn the relationships in the data.\n",
    "# Consequences: Poor performance on both training and test data, and a model that fails to capture important patterns in the data.\n",
    "# Mitigation: Increase model complexity (e.g., by adding more features or using a more complex algorithm), reduce regularization, or train the model for a longer time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aff8c80-1bd1-429b-805b-8bb4552a464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "# Answer:\n",
    "# - Cross-Validation: Use techniques like k-fold cross-validation to ensure that the model generalizes well to unseen data.\n",
    "# - Regularization: Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients and prevent the model from becoming too complex.\n",
    "# - Pruning: In decision trees, prune branches that have little significance to reduce complexity.\n",
    "# - Early Stopping: In iterative algorithms like gradient descent, stop training when the performance on a validation set starts to degrade.\n",
    "# - Data Augmentation: Increase the size of the training data by augmenting it (e.g., rotating or flipping images in image classification tasks).\n",
    "# - Dropout: In neural networks, randomly drop units during training to prevent over-reliance on any single feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb513ca-a76f-47a1-9412-009fe72d2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "# Answer:\n",
    "# Underfitting happens when a model is too simple to capture the underlying structure of the data.\n",
    "# It leads to high bias and low variance, resulting in poor performance on both the training and test data.\n",
    "# Scenarios where underfitting can occur:\n",
    "# - Using a linear model to fit non-linear data.\n",
    "# - Using too few features to train a model, resulting in a model that cannot capture the complexity of the data.\n",
    "# - Applying too much regularization, which overly simplifies the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2a2a38-ab06-4779-b071-fda2e925be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "# Answer:\n",
    "# The bias-variance tradeoff refers to the balance between two sources of error that affect model performance:\n",
    "# - Bias: The error due to overly simplistic assumptions in the model. High bias leads to underfitting and low model complexity.\n",
    "# - Variance: The error due to the model's sensitivity to fluctuations in the training data. High variance leads to overfitting and high model complexity.\n",
    "# The tradeoff: As model complexity increases, bias decreases but variance increases. The goal is to find a balance where both bias and variance are minimized, \n",
    "# achieving good generalization to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407bac2f-ee08-4f96-881b-0ff07f5d82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "# Answer:\n",
    "# - Plotting Learning Curves: By plotting training and validation error over time, you can detect overfitting (low training error, high validation error) and underfitting (both errors high).\n",
    "# - Cross-Validation: Evaluate model performance on unseen data using k-fold cross-validation to detect overfitting or underfitting.\n",
    "# - Performance Metrics: Use metrics like accuracy, precision, recall, and F1-score on both training and test sets to determine if the model is overfitting or underfitting.\n",
    "# - Comparing Train-Test Error: A large gap between training and test error indicates overfitting, while similar errors indicate underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dabc7b-4dce-45b7-91fe-f8f6d26cbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "# Answer:\n",
    "# - High Bias: Models with high bias are overly simplistic and fail to capture the complexity of the data. \n",
    "# Examples include linear regression on non-linear data and very shallow decision trees. They tend to underfit the data, leading to high error on both training and test sets.\n",
    "# - High Variance: Models with high variance are overly complex and sensitive to small fluctuations in the training data. \n",
    "# Examples include deep neural networks with no regularization and very deep decision trees. They tend to overfit the training data, leading to low training error but high test error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1780bcc6-8deb-40e9-a430-012f75229d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "# Answer:\n",
    "# Regularization is a technique used to prevent overfitting by adding a penalty to the loss function for large model parameters.\n",
    "# This penalty discourages the model from becoming too complex, leading to better generalization.\n",
    "# Common Regularization Techniques:\n",
    "# - L1 Regularization (Lasso): Adds the absolute value of the coefficients as a penalty to the loss function. It can lead to sparse models where some feature coefficients are zero.\n",
    "# - L2 Regularization (Ridge): Adds the squared value of the coefficients as a penalty to the loss function. It discourages large coefficients but does not zero them out.\n",
    "# - Dropout: In neural networks, randomly drops units (neurons) during training, preventing any single unit from being too important, reducing overfitting.\n",
    "# - Elastic Net: A combination of L1 and L2 regularization, balancing sparsity and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6348e72-1331-4503-8239-895c64009187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
