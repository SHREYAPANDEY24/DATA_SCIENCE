{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb5ac34-6470-4284-b963-261b2d08c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "# \n",
    "# Answer:\n",
    "# R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable (Y) \n",
    "# that is explained by the independent variables (X) in the regression model.\n",
    "# It is calculated as:\n",
    "# R² = 1 - (SS_res / SS_tot)\n",
    "# where:\n",
    "# - SS_res is the sum of squared residuals (difference between observed and predicted values).\n",
    "# - SS_tot is the total sum of squares (difference between observed values and the mean of observed values).\n",
    "# \n",
    "# Interpretation: R-squared values range from 0 to 1. A value of 0 indicates that the model does not explain any of the \n",
    "# variance in the dependent variable, while a value of 1 indicates that the model explains all the variance. Higher \n",
    "# R-squared values indicate a better fit of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467cc2db-e438-444a-95a5-482444bb0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "# \n",
    "# Answer:\n",
    "# Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. It \n",
    "# adjusts for the number of independent variables and only increases if the new variable improves the model more than \n",
    "# would be expected by chance.\n",
    "# It is calculated as:\n",
    "# Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
    "# where:\n",
    "# - n is the number of observations.\n",
    "# - k is the number of independent variables.\n",
    "# \n",
    "# Difference from R-squared: Regular R-squared can increase with the addition of more variables, even if they do \n",
    "# not improve the model. Adjusted R-squared penalizes the addition of non-significant predictors, making it a more \n",
    "# reliable measure for model performance, especially when dealing with multiple predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ae7639-af74-4b67-a81a-a37e07dae309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n",
    "# \n",
    "# Answer:\n",
    "# Adjusted R-squared is more appropriate to use in multiple linear regression models with more than one predictor. \n",
    "# It is particularly useful when comparing models with different numbers of independent variables, as it accounts \n",
    "# for the complexity of the model. It is a better indicator than R-squared when the model includes many predictors \n",
    "# or when you suspect that some of the predictors might not have a significant contribution to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a5480d-d70f-439b-ba7b-9de50ba53abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "# \n",
    "# Answer:\n",
    "# Mean Squared Error (MSE): The average of the squared differences between observed and predicted values.\n",
    "# MSE = (1/n) * Σ(y_i - ŷ_i)²\n",
    "# \n",
    "# Root Mean Squared Error (RMSE): The square root of the mean squared error, providing a measure of the average \n",
    "# magnitude of error.\n",
    "# RMSE = √MSE\n",
    "# \n",
    "# Mean Absolute Error (MAE): The average of the absolute differences between observed and predicted values.\n",
    "# MAE = (1/n) * Σ|y_i - ŷ_i|\n",
    "# \n",
    "# Representation:\n",
    "# - RMSE and MSE give more weight to larger errors due to squaring the differences. RMSE is in the same units as the \n",
    "# dependent variable.\n",
    "# - MAE provides a straightforward interpretation as the average absolute error, without squaring the errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41d45ed-e9de-46f7-85f7-b298cc79c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "# \n",
    "# Answer:\n",
    "# Advantages:\n",
    "# - MSE: Useful for highlighting larger errors because of the squaring effect, which is beneficial when large errors \n",
    "# are particularly undesirable.\n",
    "# - RMSE: Provides a direct interpretation of error in the same units as the dependent variable and also penalizes \n",
    "# larger errors.\n",
    "# - MAE: Easier to interpret as it represents the average error directly and is less sensitive to outliers compared to \n",
    "# MSE and RMSE.\n",
    "# \n",
    "# Disadvantages:\n",
    "# - MSE: The squaring effect can overly penalize large errors, making the metric sensitive to outliers.\n",
    "# - RMSE: Like MSE, it is sensitive to outliers and may not provide the most robust measure of model performance.\n",
    "# - MAE: Does not penalize larger errors as strongly as MSE or RMSE, which may be a drawback in some contexts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88eb26e8-7e82-4ff2-93f6-52fa3c1c2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "# \n",
    "# Answer:\n",
    "# Lasso Regularization (Least Absolute Shrinkage and Selection Operator): A regularization technique that adds a \n",
    "# penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "# Lasso Loss = RSS + λ * Σ|β_j|\n",
    "# where λ controls the amount of shrinkage.\n",
    "# \n",
    "# Difference from Ridge Regularization:\n",
    "# - Ridge Regularization penalizes the sum of the squares of the coefficients.\n",
    "# - Lasso Regularization can drive some coefficients to zero, effectively performing feature selection.\n",
    "# \n",
    "# Use Case: Lasso is more appropriate when you suspect that only a subset of features is important, and you want to \n",
    "# perform feature selection as well as regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97530a0-58af-4aa5-8cce-cd890b71b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "# \n",
    "# Answer:\n",
    "# Regularized linear models, such as Lasso and Ridge regression, add a penalty term to the loss function to constrain \n",
    "# the size of the coefficients. This helps to prevent the model from fitting the noise in the training data, which \n",
    "# reduces overfitting.\n",
    "# \n",
    "# Example: In a regression model with many features, regularization can prevent some coefficients from becoming too \n",
    "# large, which helps to avoid a model that performs well on the training data but poorly on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eaec214-0578-4ae6-b28d-d81115dc56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "# \n",
    "# Answer:\n",
    "# Limitations:\n",
    "# - Regularized linear models may not perform well if the underlying relationship between the features and the target \n",
    "# variable is highly non-linear.\n",
    "# - Regularization may lead to underfitting if the penalty is too strong, causing the model to be too simplistic.\n",
    "# - They may not handle interactions between features well, which can be important in some datasets.\n",
    "# \n",
    "# In cases where the relationships are complex or where feature interactions are significant, other modeling techniques \n",
    "# such as decision trees, random forests, or neural networks might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba6ddf-9b8f-4aa6-85bc-099b35c37cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
